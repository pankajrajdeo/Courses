# Python File I/O: Complete Guide for Biomedical AI

## 🎯 Key Takeaways

- **File I/O** enables persistent data storage - critical for saving model outputs, experimental results, and processing large biomedical datasets
- Python provides simple, elegant syntax for reading/writing files using `open()` and the `with` statement
- Understanding file modes (read, write, append) and proper resource management prevents data loss
- File I/O is fundamental for ML pipelines: loading training data, saving model checkpoints, logging experiments
- Real-world biomedical applications: processing patient records, genomic sequences, medical images, research papers

---

## 📋 Prerequisites Check

**You need to know:**
- ✅ Basic Python syntax (variables, functions, loops, conditionals)
- ✅ String manipulation and methods
- ✅ Lists and basic data structures
- ✅ Understanding of what files are at a conceptual level

**You don't need to know:**
- ❌ Advanced Python features (decorators, metaclasses)
- ❌ Operating system internals
- ❌ Binary file formats in depth
- ❌ Database systems

---

## 🧠 Intuitive Overview: Why File I/O Matters

### The Problem We're Solving

Imagine you've trained a drug discovery model that predicts protein binding affinity. You run it, get amazing results, close your laptop... and everything is **gone**. This is because:

**RAM (Random Access Memory) = Volatile Memory**
- Fast but temporary storage
- Variables exist only while program runs
- Power off = data vanishes forever

**Files = Persistent Storage**
- Saved to hard drive (HDD/SSD)
- Survives system restarts
- Enables data sharing and analysis

### Real-World Analogy

Think of RAM as a **whiteboard** - great for quick calculations, but erased at the end of class. Files are like **notebooks** - you write things down to reference later, share with colleagues, or build upon in future research.

### The Mental Model

```
┌─────────────────────────────────────────┐
│         Your Python Program             │
│                                         │
│  [Variables in RAM] ←→ [File on Disk]  │
│   (Temporary)            (Permanent)    │
│                                         │
│  Read:  Disk → RAM → Process            │
│  Write: Process → RAM → Disk            │
└─────────────────────────────────────────┘
```

---

## 🔍 Deep Dive: File I/O Fundamentals

### 1. Types of Files

**Text Files** (human-readable characters)
```
.txt  - Plain text
.csv  - Comma-separated values (datasets!)
.json - Structured data (API responses, configs)
.py   - Python source code
.log  - System/application logs
```

**Binary Files** (encoded data, not directly readable)
```
.png, .jpg - Images (medical scans, microscopy)
.mp4, .avi - Videos
.pkl       - Pickled Python objects (saved models!)
.h5, .pt   - Deep learning model weights
.npy       - NumPy arrays (efficient data storage)
```

**💡 Key Insight:** At the lowest level, ALL files are binary (0s and 1s). Text files are just binary data interpreted as characters using encoding (usually UTF-8).

---

### 2. The Basic File Operations (The File I/O Lifecycle)

```
┌─────────┐
│  OPEN   │ ← Get access to the file
└────┬────┘
     │
     ├──→ READ   (get data from file)
     ├──→ WRITE  (put data into file)
     ├──→ APPEND (add data to end)
     │
┌────┴────┐
│  CLOSE  │ ← Release file (critical!)
└─────────┘
```

**Why closing matters:**
- Flushes buffered data to disk (ensures data is saved)
- Releases system resources
- Prevents corruption if others access the file
- Good programming practice (prevents resource leaks)

---

### 3. Opening Files: The `open()` Function

**Basic Syntax:**
```python
file_object = open(filename, mode)
```

**Example:**
```python
# Open a file for reading
f = open("demo.txt", "r")

# Now you can work with f
data = f.read()

# Always close when done!
f.close()
```

#### 🔑 File Modes: Your Access Controls

| Mode | Name | What It Does | Creates if Missing? | Pointer Position | Truncates? |
|------|------|--------------|---------------------|------------------|------------|
| `r`  | Read | Read only (default) | ❌ Error | Start | ❌ |
| `w`  | Write | Write (overwrites!) | ✅ Yes | Start | ✅ Yes |
| `a`  | Append | Write at end | ✅ Yes | End | ❌ No |
| `x`  | Exclusive | Create new, fail if exists | ✅ Only new | Start | N/A |
| `r+` | Read/Write | Read and write | ❌ Error | Start | ❌ No |
| `w+` | Write/Read | Write and read | ✅ Yes | Start | ✅ Yes |
| `a+` | Append/Read | Append and read | ✅ Yes | End | ❌ No |
| `b`  | Binary | Binary mode (combine with above) | - | - | - |
| `t`  | Text | Text mode (default) | - | - | - |

**Mode Combinations:**
```python
"rb"  # Read binary (e.g., images, model files)
"wt"  # Write text (explicit, but 't' is default)
"ab"  # Append binary
"r+b" # Read/write binary
```

**🎯 Decision Tree for Choosing Mode:**

```
Need to access file?
│
├─→ YES: Does it exist?
│         │
│         ├─→ YES: Just reading?
│         │         │
│         │         ├─→ YES: Use 'r'
│         │         └─→ NO: Use 'r+' (read/write)
│         │
│         └─→ NO: Will cause error with 'r'
│
└─→ NO: Want to create/modify?
          │
          ├─→ Replace everything: Use 'w'
          ├─→ Add to end: Use 'a'
          └─→ Create only if new: Use 'x'
```

---

### 4. Reading Files: Three Methods

#### Method 1: `read()` - Read Entire File

```python
with open("demo.txt", "r") as f:
    data = f.read()  # Returns entire file as single string
    print(data)
```

**Read specific characters:**
```python
with open("demo.txt", "r") as f:
    first_5_chars = f.read(5)  # Reads first 5 characters
    print(first_5_chars)  # Output: "I am "
```

**💡 Use when:** File is small enough to fit in memory, or you need all content at once.

#### Method 2: `readline()` - Read Line by Line

```python
with open("demo.txt", "r") as f:
    line1 = f.readline()  # First line
    line2 = f.readline()  # Second line
    line3 = f.readline()  # Third line (or empty if no more lines)
    
    print(line1)
    print(line2)
```

**⚠️ Watch out:** Each line includes the newline character `\n` at the end!

**💡 Use when:** Processing large files that don't fit in memory, or when you need line-by-line processing.

#### Method 3: Iterate Directly (Most Pythonic!)

```python
with open("demo.txt", "r") as f:
    for line in f:
        # Process each line
        print(line.strip())  # .strip() removes \n
```

**💡 Use when:** You want to process each line sequentially (most common in data processing).

---

### 5. Understanding the File Pointer (Stream)

**The Invisible Cursor Concept:**

Think of reading a file like reading a book with your finger tracking your position:

```python
# File contents: "Hello World"
#                 ↑ (pointer starts here)

f.read(5)   # Reads "Hello"
#                      ↑ (pointer moves here)

f.read(6)   # Reads " World"
#                           ↑ (pointer at end)

f.read()    # Returns "" (nothing left to read!)
```

**Visualizing Pointer Movement:**

```
File: "I am learning Python"
      ↑
      Start (position 0)

After f.read(5):
File: "I am learning Python"
           ↑
           Position 5

After another f.read(8):
File: "I am learning Python"
                   ↑
                   Position 13
```

**🔍 Key Insight:** Once you've read the entire file, the pointer is at the end. Calling `read()` again returns empty string. To read again, you must:
1. Close and reopen the file, OR
2. Use `f.seek(0)` to move pointer back to start

---

### 6. Writing to Files

#### Overwrite Mode (`w`)

```python
with open("output.txt", "w") as f:
    f.write("This replaces everything!\n")
    f.write("Previous content is gone.\n")
```

**⚠️ DANGER:** Mode `w` **deletes all existing content** before writing!

#### Append Mode (`a`)

```python
with open("output.txt", "a") as f:
    f.write("This gets added to the end.\n")
    f.write("Original content preserved.\n")
```

**💡 Safe Choice:** Use `a` when you want to add to existing files.

#### Working Example with Newlines

```python
# ❌ Wrong: Lines run together
with open("data.txt", "w") as f:
    f.write("First line")
    f.write("Second line")
# Output: "First lineSecond line"

# ✅ Correct: Add \n explicitly
with open("data.txt", "w") as f:
    f.write("First line\n")
    f.write("Second line\n")
# Output:
# First line
# Second line
```

---

### 7. The `with` Statement (Best Practice!)

**Old Way (Manual Management):**
```python
f = open("file.txt", "r")
data = f.read()
f.close()  # Easy to forget! 😱
```

**New Way (Automatic Management):**
```python
with open("file.txt", "r") as f:
    data = f.read()
# File automatically closed here! ✅
```

**Why `with` is Superior:**

1. **Automatic cleanup:** File closes even if errors occur
2. **Cleaner code:** No need to remember `close()`
3. **Exception safe:** Guarantees proper resource release
4. **Pythonic:** Follows language conventions

**The Magic Behind `with`:**

```python
# What 'with' does under the hood:
f = open("file.txt", "r")
try:
    data = f.read()
finally:
    f.close()  # Runs even if error occurs!
```

**Mental Model:**

```
with open(...) as f:     ← Opens door
    # Work with file      ← Do your work
    # More work           ← More work
                          ← Door automatically locks when you leave
```

---

### 8. Deleting Files

Python doesn't have `file.delete()`. Use the `os` module instead.

```python
import os

# Delete a file
os.remove("unwanted.txt")

# Check if file exists first (safer)
if os.path.exists("maybe_exists.txt"):
    os.remove("maybe_exists.txt")
else:
    print("File doesn't exist!")
```

**⚠️ Warning:** `os.remove()` permanently deletes files. No recycle bin!

---

## 🧬 Biomedical AI Applications

### Use Case 1: Processing Clinical Trial Data

```python
"""
Reading patient data from CSV for analysis
"""
with open("patient_data.csv", "r") as f:
    # Skip header
    header = f.readline()
    
    patient_count = 0
    positive_outcomes = 0
    
    for line in f:
        patient_count += 1
        # Parse line: patient_id, treatment, outcome
        parts = line.strip().split(",")
        outcome = parts[2]
        
        if outcome == "positive":
            positive_outcomes += 1
    
    success_rate = positive_outcomes / patient_count
    print(f"Treatment success rate: {success_rate:.2%}")
```

### Use Case 2: Saving Model Training Logs

```python
"""
Logging training metrics for model development
"""
import time

def log_training_epoch(epoch, loss, accuracy):
    """Append training metrics to log file"""
    with open("training_log.txt", "a") as f:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp} | Epoch {epoch} | Loss: {loss:.4f} | Acc: {accuracy:.4f}\n"
        f.write(log_entry)

# During training loop
for epoch in range(100):
    loss = train_one_epoch()  # Your training function
    accuracy = evaluate()      # Your evaluation function
    log_training_epoch(epoch, loss, accuracy)
```

### Use Case 3: Reading Genomic Sequences (FASTA Format)

```python
"""
Parsing FASTA files - common format for DNA/protein sequences
"""
def parse_fasta(filename):
    """
    FASTA format:
    >Sequence_ID Description
    ATCGATCGATCG
    ATCGATCG
    """
    sequences = {}
    current_id = None
    current_seq = []
    
    with open(filename, "r") as f:
        for line in f:
            line = line.strip()
            
            if line.startswith(">"):  # Header line
                # Save previous sequence
                if current_id:
                    sequences[current_id] = "".join(current_seq)
                
                # Start new sequence
                current_id = line[1:].split()[0]  # Remove '>' and get ID
                current_seq = []
            else:
                current_seq.append(line)  # Add to sequence
        
        # Don't forget last sequence!
        if current_id:
            sequences[current_id] = "".join(current_seq)
    
    return sequences

# Usage
genome_sequences = parse_fasta("genome.fasta")
for seq_id, sequence in genome_sequences.items():
    print(f"{seq_id}: {len(sequence)} base pairs")
```

### Use Case 4: Saving Model Predictions

```python
"""
Writing model predictions to file for analysis
"""
def save_predictions(predictions, patient_ids, output_file):
    """
    Save model predictions with patient IDs
    Format: patient_id, prediction, confidence
    """
    with open(output_file, "w") as f:
        # Write header
        f.write("patient_id,prediction,confidence\n")
        
        # Write predictions
        for pid, (pred, conf) in zip(patient_ids, predictions):
            f.write(f"{pid},{pred},{conf:.4f}\n")

# Example usage
predictions = [
    ("positive", 0.92),
    ("negative", 0.78),
    ("positive", 0.88)
]
patient_ids = ["P001", "P002", "P003"]
save_predictions(predictions, patient_ids, "predictions.csv")
```

---

## 🎓 Practice Problems (Biomedical Context)

### Problem 1: Gene Expression Analysis

```python
"""
Create a file 'gene_expression.txt' with expression levels
Format: gene_name,expression_level
"""
with open("gene_expression.txt", "w") as f:
    f.write("BRCA1,245.3\n")
    f.write("TP53,189.7\n")
    f.write("EGFR,324.1\n")
    f.write("MYC,156.8\n")

print("Gene expression file created!")
```

### Problem 2: Replace Drug Names

```python
"""
Replace all occurrences of old drug name with new drug name in clinical notes
"""
def replace_drug_name(filename, old_drug, new_drug):
    # Read entire file
    with open(filename, "r") as f:
        data = f.read()
    
    # Replace drug names
    updated_data = data.replace(old_drug, new_drug)
    
    # Write back to file
    with open(filename, "w") as f:
        f.write(updated_data)
    
    print(f"Replaced all '{old_drug}' with '{new_drug}'")

# Usage
replace_drug_name("clinical_notes.txt", "aspirin", "acetaminophen")
```

### Problem 3: Find Patient by ID

```python
"""
Search for patient in records and return their line number
"""
def find_patient_line(filename, patient_id):
    """
    Returns: line number (1-indexed) where patient found, or -1 if not found
    """
    with open(filename, "r") as f:
        line_number = 0
        
        for line in f:
            line_number += 1
            
            # Check if patient ID in this line
            if patient_id in line:
                return line_number
        
        # Patient not found
        return -1

# Usage
result = find_patient_line("patient_records.txt", "P12345")
if result == -1:
    print("Patient not found")
else:
    print(f"Patient found on line {result}")
```

### Problem 4: Count Valid Measurements

```python
"""
From a file containing numerical measurements (comma-separated),
count how many are within valid range
"""
def count_valid_measurements(filename, min_val, max_val):
    """
    File format: value1,value2,value3,...
    """
    with open(filename, "r") as f:
        data = f.read()
    
    # Split by commas to get individual values
    measurements = data.strip().split(",")
    
    valid_count = 0
    for measurement in measurements:
        value = float(measurement)  # Convert to number
        
        if min_val <= value <= max_val:
            valid_count += 1
    
    return valid_count

# Usage: Count blood pressure readings in normal range
valid = count_valid_measurements("blood_pressure.txt", 90, 120)
print(f"Valid measurements: {valid}")
```

---

## ⚠️ Common Beginner Pitfalls

### Pitfall 1: Forgetting to Close Files

```python
# ❌ BAD: File left open
f = open("data.txt", "r")
data = f.read()
# Forgot f.close()!

# ✅ GOOD: Use 'with' statement
with open("data.txt", "r") as f:
    data = f.read()
# Automatically closed!
```

### Pitfall 2: Using Wrong Mode

```python
# ❌ BAD: Trying to write in read mode
f = open("file.txt", "r")
f.write("data")  # ERROR! File opened for reading only

# ✅ GOOD: Use write mode
with open("file.txt", "w") as f:
    f.write("data")
```

### Pitfall 3: Not Handling Missing Files

```python
# ❌ BAD: Crashes if file doesn't exist
f = open("nonexistent.txt", "r")  # FileNotFoundError!

# ✅ GOOD: Check first or handle exception
import os

if os.path.exists("file.txt"):
    with open("file.txt", "r") as f:
        data = f.read()
else:
    print("File not found!")

# Or use exception handling
try:
    with open("file.txt", "r") as f:
        data = f.read()
except FileNotFoundError:
    print("File doesn't exist!")
```

### Pitfall 4: Overwriting Important Data with `w` Mode

```python
# ❌ DANGER: This deletes all existing content!
with open("important_results.txt", "w") as f:
    f.write("New data")  # Everything else is gone!

# ✅ SAFE: Use append mode
with open("important_results.txt", "a") as f:
    f.write("New data\n")  # Adds to end
```

### Pitfall 5: Reading File Multiple Times Without Reset

```python
# ❌ PROBLEM: Second read returns empty
with open("file.txt", "r") as f:
    data1 = f.read()  # Reads entire file
    data2 = f.read()  # Returns "" (pointer at end!)

# ✅ SOLUTION: Reopen or use seek
with open("file.txt", "r") as f:
    data1 = f.read()
    f.seek(0)  # Reset pointer to start
    data2 = f.read()  # Now reads from beginning
```

### Pitfall 6: Forgetting Newline Characters

```python
# ❌ MESSY: Lines run together
with open("output.txt", "w") as f:
    f.write("Line 1")
    f.write("Line 2")
# Result: "Line 1Line 2"

# ✅ CLEAN: Add \n explicitly
with open("output.txt", "w") as f:
    f.write("Line 1\n")
    f.write("Line 2\n")
```

---

## 🔧 Advanced Patterns for ML/AI Workflows

### Pattern 1: Processing Large Files in Chunks

```python
"""
Don't load entire file into memory - process in chunks
Critical for large datasets (genomic data, imaging datasets)
"""
def process_large_file_in_chunks(filename, chunk_size=1024):
    """
    Read file in chunks to avoid memory issues
    chunk_size in bytes
    """
    with open(filename, "r") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break  # End of file
            
            # Process chunk
            process_data(chunk)  # Your processing function

# For line-by-line (most common for text data)
def process_large_file_by_lines(filename):
    with open(filename, "r") as f:
        for line in f:  # Memory efficient!
            process_line(line.strip())
```

### Pattern 2: Atomic File Writes (Prevent Corruption)

```python
"""
Write to temporary file, then rename
Prevents corruption if program crashes during write
"""
import os
import tempfile

def atomic_write(filename, data):
    """
    Write data safely - if crash occurs, original file intact
    """
    # Write to temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(filename))
    
    try:
        with os.fdopen(temp_fd, 'w') as f:
            f.write(data)
        
        # Replace original file with temp file
        os.replace(temp_path, filename)  # Atomic on most systems
    except:
        # Clean up temp file if error
        os.unlink(temp_path)
        raise

# Usage for saving model predictions
atomic_write("predictions.txt", prediction_data)
```

### Pattern 3: JSON for Structured Data

```python
"""
JSON: Better than plain text for structured data
"""
import json

# Save experiment configuration
config = {
    "model": "ResNet50",
    "learning_rate": 0.001,
    "batch_size": 32,
    "epochs": 100,
    "dataset": "protein_structures"
}

with open("experiment_config.json", "w") as f:
    json.dump(config, f, indent=2)  # indent=2 for readability

# Load configuration
with open("experiment_config.json", "r") as f:
    loaded_config = json.load(f)
    print(loaded_config["model"])  # Access as dictionary
```

---

## 📊 Visual: Complete File I/O Workflow

```
┌──────────────────────────────────────────────────────────────┐
│                    FILE I/O WORKFLOW                         │
└──────────────────────────────────────────────────────────────┘

1. READING DATA
   ┌─────────────┐
   │  File on    │
   │  Disk       │
   │ (permanent) │
   └──────┬──────┘
          │
          │ open("file.txt", "r")
          ↓
   ┌──────────────┐
   │ File Object  │ ←─ with statement manages this
   │   (stream)   │
   └──────┬───────┘
          │
          │ .read() / .readline()
          ↓
   ┌──────────────┐
   │ Data in RAM  │ ←─ Your variable
   │  (temporary) │
   └──────────────┘


2. WRITING DATA
   ┌──────────────┐
   │ Data in RAM  │ ←─ Your data
   └──────┬───────┘
          │
          │ open("file.txt", "w")
          ↓
   ┌──────────────┐
   │ File Object  │
   │   (stream)   │
   └──────┬───────┘
          │
          │ .write(data)
          ↓
   ┌──────────────┐
   │  File on     │
   │  Disk        │
   │ (permanent)  │
   └──────────────┘


3. FILE MODES DECISION TREE

   Opening a file?
   │
   ├─→ Read only? ─────→ Use 'r'
   │
   ├─→ Modify existing? ───┬─→ Replace all? ─→ Use 'w'
   │                       └─→ Add to end? ──→ Use 'a'
   │
   └─→ Read AND write? ────┬─→ From start? ──→ Use 'r+'
                           ├─→ Replace all? ─→ Use 'w+'
                           └─→ From end? ────→ Use 'a+'
```

---

## 🎯 What You Just Learned

### Core Concepts
✅ File I/O enables persistent data storage (survives program termination)  
✅ `open()` function with modes controls file access  
✅ `with` statement ensures proper resource management  
✅ Text vs binary files serve different purposes  
✅ File pointer tracks position during read/write operations  

### Practical Skills
✅ Read files with `read()`, `readline()`, or iteration  
✅ Write/append data with `write()`  
✅ Choose appropriate file modes for different tasks  
✅ Delete files with `os.remove()`  
✅ Process structured data (CSV, FASTA formats)  

### Biomedical AI Context
✅ Load training datasets from files  
✅ Save model predictions and logs  
✅ Process genomic sequences and clinical data  
✅ Implement robust data processing pipelines  

---

## 📚 Next Steps & Resources

### Immediate Practice
1. **Build a data loader:** Read a CSV file with patient data, compute statistics
2. **Create a logging system:** Write function to log events with timestamps
3. **Parse real biomedical data:** Download a FASTA file, count sequences
4. **Implement checkpoint saving:** Save/load model state (use `json` or `pickle`)

### Level Up Your Skills
- **CSV module:** Built-in Python module for robust CSV handling
- **Pandas:** Industry standard for tabular data (reads CSV, Excel, SQL)
- **NumPy:** `.npy` format for efficient array storage
- **HDF5/NetCDF:** Formats for large scientific datasets
- **Pickle:** Serialize Python objects (save model states)

### Essential Reading
- Python Official Docs: [File I/O](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files)
- Real Python: [Reading and Writing Files in Python](https://realpython.com/read-write-files-python/)
- BioPython: [Sequence I/O](https://biopython.org/wiki/SeqIO) for genomic data

### Biomedical-Specific Resources
- **BioPython:** Standard library for computational biology
- **SimpleITK/nibabel:** Medical image file handling
- **PubMed Parser:** Extract data from biomedical literature
- **scanpy:** Single-cell genomics data processing

### Common File Formats in Biomedical AI
```
Genomics: .fasta, .fastq, .vcf, .sam, .bam
Imaging:  .dcm (DICOM), .nii (NIfTI), .tiff
Data:     .csv, .tsv, .h5ad (AnnData), .zarr
Models:   .h5, .pt, .pth, .ckpt, .pb
```

---

## 💡 Pro Tips for Biomedical AI Development

1. **Always validate file formats:** Medical data often has inconsistencies
2. **Log everything:** Track data processing steps for reproducibility
3. **Use checksums:** Verify data integrity (`hashlib` module)
4. **Implement error handling:** Files can be corrupted or missing
5. **Consider privacy:** Anonymize patient data before saving
6. **Version your data:** Include timestamps or version numbers in filenames
7. **Compress large files:** Use `.gz` for text, `.npz` for NumPy arrays
8. **Document file formats:** Write README files explaining your data structure

**Remember:** In biomedical AI, data integrity is paramount. A single corrupted file can invalidate months of research. Always implement robust file handling with proper error checking!

---

## 🎓 Summary

File I/O is the **bridge between temporary computations and permanent storage**. For biomedical AI:

- It's how you **load training data** (patient records, genomic sequences, medical images)
- It's how you **save model outputs** (predictions, trained weights, experiment logs)
- It's how you **ensure reproducibility** (logging configurations, saving intermediate results)

Master file I/O, and you've mastered the foundation of production ML systems. Next up: dive deeper into data processing libraries (Pandas, NumPy) and learn about efficient data formats for large-scale biomedical datasets!

**Keep practicing, keep building, and remember:** Every great AI system starts with solid file I/O! 🚀
